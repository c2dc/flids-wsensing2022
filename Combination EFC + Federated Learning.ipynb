{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "395aed2b-bbc7-44ec-ab46-aca9b5f4c784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leohmelo/venv/lib/python3.10/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/leohmelo/venv/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/leohmelo/venv/lib/python3.10/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/leohmelo/venv/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import efc as efc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from efc import EnergyBasedFlowClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\n",
    "\n",
    "verbose = False\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f6f3005-6053-417b-a948-41033f862e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "silos_path = \"./datasets/silos/\"\n",
    "silos_files = [f for f in listdir(silos_path) if isfile(join(silos_path, f))]\n",
    "\n",
    "silos = {}\n",
    "for silo in silos_files:\n",
    "    silo_name = silo.replace(\".csv.gz\", \"\")\n",
    "    silos[silo_name] = pd.read_csv(silos_path + silo)\n",
    "    \n",
    "    if(verbose):\n",
    "        print(f\"{silo_name} >>> \\n{silos[silo_name]['Label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bcd2bc5-a076-4042-9f1f-3f3d214ec4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    cols = X.select_dtypes(include=[np.float64]).columns\n",
    "    X[cols] = X[cols].astype(np.float32)\n",
    "\n",
    "    X.fillna(0, inplace=True)\n",
    "    X.replace(np.inf, 9999, inplace=True)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f735e443-d5ce-4a6c-894c-662fcf6c773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45d2caaf-a819-478e-881f-044d5576b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced = True \n",
    "def rebalance_agent(agent):\n",
    "    agent_label_count = agent[\"Label\"].value_counts().to_dict()\n",
    "    benign_count = agent_label_count[0] \n",
    "    malicious_count = agent_label_count[1]\n",
    "    \n",
    "    if(balanced):\n",
    "        if (benign_count > malicious_count):\n",
    "            downsampled_0 = resample(\n",
    "                                agent[agent.Label == 0], \n",
    "                                replace=False, \n",
    "                                n_samples=agent[agent.Label == 1].shape[0],\n",
    "                                random_state=1337\n",
    "                            )\n",
    "            agent_balanced = pd.concat([downsampled_0, agent[agent.Label == 1]])\n",
    "        elif (benign_count < malicious_count): \n",
    "            downsampled_1 = resample(\n",
    "                                agent[agent.Label == 1], \n",
    "                                replace=False, \n",
    "                                n_samples=agent[agent.Label == 0].shape[0], \n",
    "                                random_state=1337\n",
    "                            )\n",
    "            agent_balanced = pd.concat([downsampled_1, agent[agent.Label == 0]])\n",
    "        else:\n",
    "            agent_balanced = agent\n",
    "    else:\n",
    "        agent_balanced = agent\n",
    "    \n",
    "    return agent_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21e76f5b-9ed3-4712-87d4-1a9c04be573e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_to_delete = [\n",
    "     'DST_TO_SRC_SECOND_BYTES',\n",
    "     'FTP_COMMAND_RET_CODE',\n",
    "     'RETRANSMITTED_IN_PKTS',\n",
    "     'RETRANSMITTED_IN_BYTES',\n",
    "     'DNS_TTL_ANSWER',\n",
    "     'SERVER_TCP_FLAGS',\n",
    "     'DNS_QUERY_TYPE',\n",
    "     'DNS_QUERY_ID',\n",
    "     'SRC_TO_DST_SECOND_BYTES',\n",
    "     'Unnamed: 0',\n",
    "     'SRC_TO_DST_AVG_THROUGHPUT',\n",
    "     'MIN_TTL',\n",
    "     'SHORTEST_FLOW_PKT',\n",
    "     'IN_BYTES',\n",
    "     'MAX_TTL',\n",
    "     'LONGEST_FLOW_PKT',\n",
    "     'MAX_IP_PKT_LEN',\n",
    "     'RETRANSMITTED_OUT_PKTS',\n",
    "     'NUM_PKTS_128_TO_256_BYTES',\n",
    "     'OUT_PKTS',\n",
    "     'RETRANSMITTED_OUT_BYTES',\n",
    "     'MIN_IP_PKT_LEN',\n",
    "     'NUM_PKTS_256_TO_512_BYTES',\n",
    "     'DURATION_IN',\n",
    "     'Label'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03410221-017d-4840-a451-d3b7b030ee6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "siloed_feat_efc = {}\n",
    "\n",
    "for key, silo in silos.items():\n",
    "    \n",
    "    X = silo.drop(columns=features_to_delete).copy()\n",
    "    X = preprocess(X)\n",
    "    y = silo[\"Label\"].copy()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, random_state=42, stratify=y, shuffle=True, test_size=0.3\n",
    "    )\n",
    "    \n",
    "    X_train[\"Label\"] = y_train \n",
    "    X_train = rebalance_agent(X_train) \n",
    "    y_train = X_train[\"Label\"] \n",
    "    X_train = X_train.drop(columns=\"Label\").copy() \n",
    "    \n",
    "    # train and test EFC\n",
    "    clf = EnergyBasedFlowClassifier(cutoff_quantile=0.95)\n",
    "    print(y_train.value_counts())\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    siloed_feat_efc[key] = clf\n",
    "    \n",
    "    y_pred, y_energies = clf.predict(X_test, return_energies=True)\n",
    "    \n",
    "    print(key, \"| cutoff =\", clf.estimators_[0].cutoff_, \"\\n-------------------------------------------------------------------\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    # ploting energies\n",
    "    benign = np.where(y_test == 0)[0]\n",
    "    malicious = np.where(y_test == 1)[0]\n",
    "\n",
    "    benign_energies = y_energies[benign]\n",
    "    malicious_energies = y_energies[malicious]\n",
    "    cutoff = clf.estimators_[0].cutoff_\n",
    "\n",
    "    bins = np.histogram(y_energies, bins=60)[1]\n",
    "\n",
    "    plt.hist(\n",
    "        malicious_energies,\n",
    "        bins,\n",
    "        facecolor=\"#006680\",\n",
    "        alpha=0.7,\n",
    "        ec=\"white\",\n",
    "        linewidth=0.3,\n",
    "        label=\"malicious\",\n",
    "    )\n",
    "    plt.hist(\n",
    "        benign_energies,\n",
    "        bins,\n",
    "        facecolor=\"#b3b3b3\",\n",
    "        alpha=0.7,\n",
    "        ec=\"white\",\n",
    "        linewidth=0.3,\n",
    "        label=\"benign\",\n",
    "    )\n",
    "    plt.axvline(cutoff, color=\"r\", linestyle=\"dashed\", linewidth=1)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel(\"Energy\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187edebd-904a-487c-9683-46d7a0b92c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_feat = {}\n",
    "full_feat = []\n",
    "avg_feat_info = {} \n",
    "\n",
    "for model_key, model in siloed_feat_efc.items():\n",
    "    \n",
    "    silos_y_test = np.array([]) \n",
    "    silos_y_pred = np.array([]) \n",
    "    \n",
    "    for key, silo in silos.items():\n",
    "        #if model_key == key:\n",
    "        #    continue\n",
    "            \n",
    "        X = silo.drop(columns=features_to_delete).copy()\n",
    "        #X.dropna(inplace=True)\n",
    "        X = preprocess(X)\n",
    "        y = silo[\"Label\"].copy()\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, random_state=42, stratify=y, shuffle=True, test_size=0.3\n",
    "        )\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        score = f1_score(y_test, y_pred)\n",
    "        \n",
    "        silos_y_test = np.concatenate((silos_y_test, y_test), axis=None)\n",
    "        silos_y_pred = np.concatenate((silos_y_pred, y_pred), axis=None)\n",
    "        \n",
    "        data_feat[key] = score\n",
    "    \n",
    "    print(f\"{model_key} >>> \")\n",
    "    print(confusion_matrix(silos_y_test, silos_y_pred))\n",
    "        \n",
    "    full_feat.append(data_feat)\n",
    "    avg_feat_info[model_key] = data_feat\n",
    "    data_feat = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee226a11-ea33-4ee1-8c8a-61b5c46109f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cross_silo_feat_efc = pd.DataFrame(full_feat, index=list(siloed_feat_efc.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad91733f-d342-4b80-ad21-835468b3142b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,16))\n",
    "ax = sns.heatmap(cross_silo_feat_efc, xticklabels=cross_silo_feat_efc.columns, yticklabels=cross_silo_feat_efc.columns, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e07e9b7-5ce8-45b3-8c2d-10ba3d604877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculo da média do modelo de cada um dos agentes avaliado em outros datasets\n",
    "average_f1_feat_score = dict()\n",
    "\n",
    "for i in range(len(avg_feat_info)):\n",
    "    name = \"agent\" + str(i+1)\n",
    "    average_f1_feat_score[name] = average(avg_feat_info[name].values())\n",
    "\n",
    "for name, value in average_f1_feat_score.items():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344fe66d-0a91-4824-85f4-0369b8d893a8",
   "metadata": {},
   "source": [
    "## Validação entre os silos utilizando feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4967e289-5e3f-45f0-b2e1-ce02a4851da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = silos.copy()\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ea5ce7-0c8f-4ffb-820d-889134ea6199",
   "metadata": {},
   "source": [
    "### FedAvg hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd48fd6-68b4-4477-843b-e583447a5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 50\n",
    "\n",
    "# Epochs\n",
    "# FedAvg requires a minimum number of training epochs before averaging\n",
    "# For epoch=1 is the same as FedSGD (https://www.cs.cornell.edu/~shmat/shmat_ccs15.pdf)\n",
    "epochs = 20\n",
    "\n",
    "# train_size same as batch size\n",
    "# If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. \n",
    "# If int, represents the absolute number of train samples\n",
    "train_size=100 #0.05\n",
    "\n",
    "remove_agents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7787b1-7bc6-474f-b3f0-4d63adf654e2",
   "metadata": {},
   "source": [
    "### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dca34e-5e4b-4a13-a512-b54d791a5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(results, remove_list=[]):\n",
    "    all_values = []\n",
    "    for major_key in results.keys():\n",
    "        if (major_key in remove_list) in remove_list:\n",
    "            continue\n",
    "        values = []\n",
    "        for key in agents.keys():\n",
    "            if key in remove_list:\n",
    "                continue\n",
    "            values.append(results[major_key][key])\n",
    "        all_values.append(values)\n",
    "    return all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2a3dd-7321-4855-828c-21a70dc39f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update each agent model by current global model values\n",
    "def load_global_model(model):\n",
    "    model.intercept_ = fedavg.intercept_.copy()\n",
    "    model.coef_ = fedavg.coef_.copy()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c80e29a-10af-4dcb-8730-e9c87cfbba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_global_model(applicable_models, round_weights):\n",
    "    # Average models parameters\n",
    "    coefs = []\n",
    "    intercept = []\n",
    "    for model in applicable_models:\n",
    "        coefs.append(model.coef_)\n",
    "        intercept.append(model.intercept_)\n",
    "    \n",
    "    # average and update FedAvg (aggregator model)\n",
    "    fedavg.coef_ = np.average(coefs, axis=0, weights=round_weights) # weight\n",
    "    fedavg.intercept_ = np.average(intercept, axis=0, weights=round_weights) # weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e471ec9-a8fe-4232-a2c1-2a86df8a90ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_agent(X, y, model, epochs, class_weight):\n",
    "    for _ in range(0, epochs):\n",
    "        model.partial_fit(X, y, classes=np.unique(y), sample_weight=class_weight)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b63cc4f-b1b1-4c21-a96c-128ef116b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update each agent model by current global model values\n",
    "def load_global_model(model):\n",
    "    model.intercept_ = fedavg.intercept_.copy()\n",
    "    model.coef_ = fedavg.coef_.copy()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d6354-3886-4d69-80e0-65becaf601b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_global_model(applicable_models, round_weights):\n",
    "    # Average models parameters\n",
    "    coefs = []\n",
    "    intercept = []\n",
    "    for model in applicable_models:\n",
    "        coefs.append(model.coef_)\n",
    "        intercept.append(model.intercept_)\n",
    "    \n",
    "    # average and update FedAvg (aggregator model)\n",
    "    fedavg.coef_ = np.average(coefs, axis=0, weights=round_weights) # weight\n",
    "    fedavg.intercept_ = np.average(intercept, axis=0, weights=round_weights) # weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe4824-eac7-49ce-9136-1d42ea1da2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_agent(X, y, model, epochs, class_weight):\n",
    "    for _ in range(0, epochs):\n",
    "        model.partial_fit(X, y, classes=np.unique(y), sample_weight=class_weight)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2e944-3600-43aa-9d9b-2e0f3b2e37f2",
   "metadata": {},
   "source": [
    "### Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf642fd-e0e2-4e3d-9215-c4a6c02a346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fedavg = SGDClassifier() \n",
    "lr = 0.1\n",
    "def initialize_global_model(agent, features_to_drop):\n",
    "    lr = 0.1\n",
    "    fedavg = SGDClassifier(n_jobs=-1, random_state=seed, loss=\"log\", learning_rate='constant', eta0=lr, verbose=0) # global\n",
    "    features = agent.drop(columns=features_to_drop).shape[1] # total number of features\n",
    "    fedavg.intercept_ = np.zeros(1)\n",
    "    fedavg.coef_ = np.zeros((1, features))\n",
    "    fedavg.classes_ = np.array([0, 1])\n",
    "    return fedavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08d401-d33d-409f-bb8c-c1fbd04e5c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_feature = dict()\n",
    "for agent in agents.keys():\n",
    "    sgd_feature[agent] = SGDClassifier(n_jobs=-1, random_state=seed, loss=\"log\", learning_rate='constant', eta0=lr, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a6ed5c-faa1-4184-96cd-0e5c4d167740",
   "metadata": {},
   "outputs": [],
   "source": [
    "fedavg = initialize_global_model(agents[\"agent1\"], features_to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b92257-b27d-4f9d-b257-a6845f830843",
   "metadata": {},
   "source": [
    "### Benign Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a420118-10f9-43ff-bbf8-d3d6a783debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds_f1_score = dict() \n",
    "\n",
    "print(\"Federated Training Started.\")\n",
    "for r in range(rounds):\n",
    "    sample_size = random.randint(int(len(agents)/2), len(agents))\n",
    "    applicable_agents = random.sample(list(agents.keys()), k=sample_size) # select random agents/silos\n",
    "\n",
    "    applicable_models = []\n",
    "    applicable_name = []\n",
    "\n",
    "    round_weights = []\n",
    "    dataset_size = 0\n",
    "\n",
    "    for name, agent in agents.items():\n",
    "        if name not in applicable_agents: # check if agent/silos is in applicable_agents\n",
    "            continue\n",
    "\n",
    "        if name in remove_agents: # check if agent/silos is in remove_agents, if yes, jump (refer to FedAvg hyperparams)\n",
    "            continue\n",
    "        \n",
    "        prep = StandardScaler() \n",
    "        applicable_name.append(name)\n",
    "\n",
    "        agent_balanced = rebalance_agent(agent)\n",
    "        \n",
    "        print(\".\", end=\" \")\n",
    "        X = agent_balanced.drop(columns=features_to_delete)\n",
    "        y = agent_balanced.Label\n",
    "      \n",
    "        X = preprocess(X)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, train_size=train_size, stratify=y) #stratified / random_state=seed\n",
    "        X_train = prep.fit_transform(X_train)\n",
    "\n",
    "        dataset_size += X_train.shape[0]\n",
    "\n",
    "        sample_weights = compute_sample_weight('balanced', y=y_train)\n",
    "        sgd_feature[name] = load_global_model(sgd_feature[name])\n",
    "        sgd_feature[name] = train_local_agent(X_train, y_train, sgd_feature[name], epochs, sample_weights)\n",
    "        round_weights.append(X_train.shape[0])\n",
    "        applicable_models.append(sgd_feature[name])\n",
    "\n",
    "    round_weights = np.array(round_weights) / dataset_size # calculate weight based on actual dataset size\n",
    "\n",
    "    try:\n",
    "        update_global_model(applicable_models, round_weights)\n",
    "    except:\n",
    "        print(\"Error updating global model, due to division by 0\")\n",
    "    \n",
    "    for name, agent in agents.items():\n",
    "        sgd_feature[name] = load_global_model(sgd_feature[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3be089-cf8f-4891-a3dc-1108b1cb6221",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_results = dict()\n",
    "\n",
    "print(\"Cross Validation Analisys Started...\")\n",
    "for name, agent in agents.items():\n",
    "    \n",
    "    silos_y_test = np.array([]) \n",
    "    silos_y_pred = np.array([])\n",
    "    \n",
    "    print(\".\", end=\"\")\n",
    "    if name in remove_agents:\n",
    "            continue\n",
    "            \n",
    "    agent_balanced = rebalance_agent(agent) \n",
    "    \n",
    "    X = agent_balanced.drop(columns=features_to_delete)\n",
    "    y = agent_balanced.Label\n",
    "\n",
    "    X = preprocess(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y) #stratified\n",
    "    \n",
    "    X_train = prep.fit_transform(X_train)\n",
    "    X_test = prep.transform(X_test) \n",
    "    \n",
    "    y_pred = sgd_feature[name].predict(X_test)\n",
    "    \n",
    "    cross_validation_results[name] = dict()\n",
    "    cross_validation_results[name][name] = f1_score(y_test, y_pred, average=\"binary\")\n",
    "    \n",
    "    print(name + \" \" + str(cross_validation_results[name][name]))\n",
    "    \n",
    "    silos_y_test = np.concatenate((silos_y_test, y_test), axis=None)\n",
    "    silos_y_pred = np.concatenate((silos_y_pred, y_pred), axis=None)\n",
    "    \n",
    "    for test_name, test_agent in agents.items():\n",
    "        if test_name == name:\n",
    "            continue\n",
    "        \n",
    "        X = test_agent.drop(columns=features_to_delete)\n",
    "        y = test_agent.Label\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y) #stratified\n",
    "        X_test = preprocess(X_test)\n",
    "        X_test = prep.transform(X_test)\n",
    "        \n",
    "        y_pred = sgd_feature[name].predict(X_test)\n",
    "        cross_validation_results[name][test_name] = f1_score(y_test, y_pred, average=\"binary\")\n",
    "        \n",
    "        silos_y_test = np.concatenate((silos_y_test, y_test), axis=None)\n",
    "        silos_y_pred = np.concatenate((silos_y_pred, y_pred), axis=None)\n",
    "    \n",
    "    print(f\"{name} >>> \")\n",
    "    print(confusion_matrix(silos_y_test, silos_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284a088-39a1-43b2-8a1e-6f2575d15347",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_results = process_results(cross_validation_results, remove_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e0c66-277a-4a69-9c2c-34dbb52913d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i for i in agents.keys() if i not in remove_agents]\n",
    "plt.figure(figsize = (30,15))\n",
    "ax = sns.heatmap(feature_results, vmin=0, vmax=1, annot=True, fmt=\".4f\", xticklabels=labels, yticklabels=labels, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9847a4-7479-4a75-a00c-2f6e46ee7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_f1_score = dict()\n",
    "for i, name in enumerate(agents.keys()):\n",
    "    average_f1_score[name] = average(feature_results[i])\n",
    "\n",
    "for value in average_f1_score.values():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b6deff-c49c-4754-a083-64d9feba948b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation models by combining EFC + Federated Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2acbf52-1472-4e64-b9c0-eb1a5167e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925de63a-4cfd-44f7-8419-b8b735fa3943",
   "metadata": {},
   "source": [
    "Combination of the models using **OR** operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3408916-96ef-43c8-99db-b0f4291fc6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_results = dict()\n",
    "\n",
    "print(\"Cross Validation Analisys Started...\")\n",
    "for name, agent in agents.items():\n",
    "    \n",
    "    silos_y_test = np.array([]) \n",
    "    silos_y_pred = np.array([])\n",
    "    \n",
    "    print(\".\", end=\"\")\n",
    "    if name in remove_agents:\n",
    "            continue\n",
    "            \n",
    "    agent_balanced = rebalance_agent(agent) \n",
    "    \n",
    "    X = agent_balanced.drop(columns=features_to_delete)\n",
    "    y = agent_balanced.Label\n",
    "\n",
    "    X = preprocess(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y) #stratified\n",
    "    \n",
    "    X_train = prep.fit_transform(X_train)\n",
    "    X_test = prep.transform(X_test) \n",
    "    \n",
    "    y_pred_federated = sgd_feature[name].predict(X_test)\n",
    "    y_pred_efc = siloed_feat_efc[name].predict(X_test) \n",
    "    \n",
    "    y_pred = np.logical_or(y_pred_federated, y_pred_efc)\n",
    "    \n",
    "    silos_y_test = np.concatenate((silos_y_test, y_test), axis=None)\n",
    "    silos_y_pred = np.concatenate((silos_y_pred, y_pred), axis=None)\n",
    "    \n",
    "    cross_validation_results[name] = dict()\n",
    "    cross_validation_results[name][name] = f1_score(y_test, y_pred, average=\"binary\")\n",
    "    \n",
    "    print(name + \" \" + str(cross_validation_results[name][name]))\n",
    "    \n",
    "    for test_name, test_agent in agents.items():\n",
    "        if test_name == name:\n",
    "            continue\n",
    "        \n",
    "        X = test_agent.drop(columns=features_to_delete)\n",
    "        y = test_agent.Label\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y) #stratified\n",
    "        X_test = preprocess(X_test)\n",
    "        X_test = prep.transform(X_test)\n",
    "        \n",
    "        y_pred_federated = sgd_feature[name].predict(X_test)\n",
    "        y_pred_efc = siloed_feat_efc[name].predict(X_test)\n",
    "        \n",
    "        y_pred = np.logical_or(y_pred_federated, y_pred_efc)\n",
    "        \n",
    "        silos_y_test = np.concatenate((silos_y_test, y_test), axis=None)\n",
    "        silos_y_pred = np.concatenate((silos_y_pred, y_pred), axis=None)\n",
    "        \n",
    "        cross_validation_results[name][test_name] = f1_score(y_test, y_pred, average=\"binary\")\n",
    "    \n",
    "    print(f\"{name} >>> \")\n",
    "    cm = confusion_matrix(silos_y_test, silos_y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4efb82-1459-49cc-b16f-b2902bfea17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_valores_01 = np.array([0,0,0])\n",
    "array_valores_02 = np.array([1,0,0])\n",
    "\n",
    "print(np.logical_or(array_valores_01, array_valores_02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efe442b-8485-4f9d-b8df-792d9cc56316",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_results = process_results(cross_validation_results, remove_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1e6a9e-4e42-46e5-bfa7-f52a89941795",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i for i in agents.keys() if i not in remove_agents]\n",
    "plt.figure(figsize = (30,15))\n",
    "ax = sns.heatmap(feature_results, vmin=0, vmax=1, annot=True, fmt=\".4f\", xticklabels=labels, yticklabels=labels, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae28ee-a000-4c05-a2ca-fd54429bcd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_f1_score = dict()\n",
    "for i, name in enumerate(agents.keys()):\n",
    "    average_f1_score[name] = average(feature_results[i])\n",
    "\n",
    "for value in average_f1_score.values():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a7a439-bbf9-4aec-9a17-965228875707",
   "metadata": {},
   "source": [
    "Combination of the models using **AND** operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd97048-7167-4ea5-8039-eae39da9aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_results = dict()\n",
    "\n",
    "print(\"Cross Validation Analisys Started...\")\n",
    "for name, agent in agents.items():\n",
    "    \n",
    "    silos_y_test = np.array([]) \n",
    "    silos_y_pred = np.array([])\n",
    "    \n",
    "    print(\".\", end=\"\")\n",
    "    if name in remove_agents:\n",
    "            continue\n",
    "            \n",
    "    agent_balanced = rebalance_agent(agent) \n",
    "    \n",
    "    X = agent_balanced.drop(columns=features_to_delete)\n",
    "    y = agent_balanced.Label\n",
    "\n",
    "    X = preprocess(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y) #stratified\n",
    "    \n",
    "    X_train = prep.fit_transform(X_train)\n",
    "    X_test = prep.transform(X_test) \n",
    "    \n",
    "    y_pred_federated = sgd_feature[name].predict(X_test)\n",
    "    y_pred_efc = siloed_feat_efc[name].predict(X_test) \n",
    "    \n",
    "    y_pred = np.logical_and(y_pred_federated, y_pred_efc)\n",
    "    \n",
    "    silos_y_test = np.concatenate((silos_y_test, y_test), axis=None)\n",
    "    silos_y_pred = np.concatenate((silos_y_pred, y_pred), axis=None)\n",
    "\n",
    "    cross_validation_results[name] = dict()\n",
    "    cross_validation_results[name][name] = f1_score(y_test, y_pred, average=\"binary\")\n",
    "    \n",
    "    print(name + \" \" + str(cross_validation_results[name][name]))\n",
    "    \n",
    "    for test_name, test_agent in agents.items():\n",
    "        if test_name == name:\n",
    "            continue\n",
    "        \n",
    "        X = test_agent.drop(columns=features_to_delete)\n",
    "        y = test_agent.Label\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y) #stratified\n",
    "        X_test = preprocess(X_test)\n",
    "        X_test = prep.transform(X_test)\n",
    "        \n",
    "        y_pred_federated = sgd_feature[name].predict(X_test)\n",
    "        y_pred_efc = siloed_feat_efc[name].predict(X_test)\n",
    "        \n",
    "        y_pred = np.logical_and(y_pred_federated, y_pred_efc)\n",
    "        \n",
    "        silos_y_test = np.concatenate((silos_y_test, y_test), axis=None)\n",
    "        silos_y_pred = np.concatenate((silos_y_pred, y_pred), axis=None)\n",
    "        \n",
    "        cross_validation_results[name][test_name] = f1_score(y_test, y_pred, average=\"binary\")\n",
    "    \n",
    "    print(f\"{name} >>> \")\n",
    "    cm = confusion_matrix(silos_y_test, silos_y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c349c5-576a-4c22-ac6a-dfc29d46f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_results = process_results(cross_validation_results, remove_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd36b6-aa26-4b4d-a81d-b9d1c21b8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i for i in agents.keys() if i not in remove_agents]\n",
    "plt.figure(figsize = (30,15))\n",
    "ax = sns.heatmap(feature_results, vmin=0, vmax=1, annot=True, fmt=\".4f\", xticklabels=labels, yticklabels=labels, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7a5c8-f2c5-4087-a9a1-bd6418d31c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_f1_score = dict()\n",
    "for i, name in enumerate(agents.keys()):\n",
    "    average_f1_score[name] = average(feature_results[i])\n",
    "\n",
    "for value in average_f1_score.values():\n",
    "    print(value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
